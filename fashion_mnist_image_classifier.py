# -*- coding: utf-8 -*-
"""Fashion-MNIST Image Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1947KJTT_CL_IiBOOS3b8UePj0Q7EKdUL
"""

import numpy as np

from tensorflow.keras.datasets import fashion_mnist

data = fashion_mnist.load_data()

from sklearn.model_selection import train_test_split

(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)

X_train = X_train.reshape(-1,28*28) /255.0
X_test = X_test.reshape(-1,28*28) /255.0

np.size(np.unique(y_train))

input_size = 784
hidden1 = 196
hidden2 = 64
output_size = 10
epochs = 25
batch_size = 64
learning_rate = 0.2
learning_rate_decay = 0.95
patience = 5
best_accuracy = 0
best_params = None
lambd = 0.01
t = 0
optimizer = None

class MNISTClassifier:
    def __init__(self, input_size, hidden1, hidden2, output_size, learning_rate=0.01, lambd=0.01):
        self.lr = learning_rate
        self.lambd = lambd
        self.input_size = input_size
        self.hidden1 = hidden1
        self.hidden2 = hidden2
        self.output_size = output_size
        self.parameters = {}
        self.v, self.s = None, None
        self.t = 1
        self.train_losses = []
        self.train_accuracies = []
        self.test_accuracies = []
        self.best_accuracy = 0
        self.best_params = None
        self.epochs_no_improve = 0

    def initialize_params(self, layer_size):
        L = len(layer_size) - 1
        for l in range(1, L + 1):
            self.parameters['W' + str(l)] = np.random.randn(layer_size[l], layer_size[l - 1]) * np.sqrt(2.0 / layer_size[l - 1])
            self.parameters['b' + str(l)] = np.zeros((layer_size[l], 1))

    def apply_dropout_fwd(self, A, keep_prob):
        d = (np.random.rand(*A.shape) < keep_prob).astype(float)
        A *= d
        A /= keep_prob
        return A, d

    def apply_dropout_bwd(self, dA, d, keep_prob):
        dA *= d
        dA /= keep_prob
        return dA

    def activation_function(self, activation, Z):
        if activation == 'relu':
            A = np.maximum(0, Z)
        elif activation == 'softmax':
            Z_shifted = Z - np.max(Z, axis=0, keepdims=True)
            A = np.exp(Z_shifted) / np.sum(np.exp(Z_shifted), axis=0, keepdims=True)
        elif activation == 'tanh':
            A = np.tanh(Z)
        return A, Z

    def linear_fwd_prop(self, A, W, b):
        Z = np.dot(W, A) + b
        return Z, (A, W, b)

    def forward_propagation(self, X, parameters, training=True):
        caches = []
        A = X
        L = len(parameters) // 2

        for l in range(1, L):
            Z, lin_cache = self.linear_fwd_prop(A, parameters['W' + str(l)], parameters['b' + str(l)])
            A, act_cache = self.activation_function('relu', Z)
            if training:
                A, d = self.apply_dropout_fwd(A, 0.5)
            else:
                d = None
            caches.append((lin_cache, act_cache, d))

        Z, lin_cache = self.linear_fwd_prop(A, parameters['W' + str(L)], parameters['b' + str(L)])
        A, act_cache = self.activation_function('softmax', Z)
        caches.append((lin_cache, act_cache, None))
        return A, caches

    def cost_func(self, y, yhat, parameters=None, lambd=0.0):
        m = y.shape[1]
        loss = -np.sum(y * np.log(yhat + 1e-8)) / m
        L2_term = 0
        if parameters:
            L = len(parameters) // 2
            for l in range(1, L + 1):
                L2_term += np.sum(np.square(parameters['W' + str(l)]))
            L2_term *= lambd / (2 * m)
        return loss + L2_term

    def relu_backward(self, dA, Z):
        return dA * (Z > 0)

    def back_prop(self, y, yhat, caches, parameters, lambd=0.0, grad_clip_value=5):
        grads = {}
        L = len(caches)
        m = y.shape[1]
        dZ = yhat - y
        dZ = np.clip(dZ, -grad_clip_value, grad_clip_value)

        A_prev, W, b = caches[L - 1][0]
        grads['dW' + str(L)] = (dZ @ A_prev.T) / m + (lambd / m) * W
        grads['db' + str(L)] = np.sum(dZ, axis=1, keepdims=True) / m
        dA_prev = W.T @ dZ

        for l in reversed(range(L - 1)):
            A_prev, W, b = caches[l][0]
            Z = caches[l][1]
            d = caches[l][2]
            dA = self.apply_dropout_bwd(dA_prev, d, 0.5) if d is not None else dA_prev
            dZ = self.relu_backward(dA, Z)
            dZ = np.clip(dZ, -grad_clip_value, grad_clip_value)
            grads['dW' + str(l + 1)] = (dZ @ A_prev.T) / m + (lambd / m) * W
            grads['db' + str(l + 1)] = np.sum(dZ, axis=1, keepdims=True) / m
            dA_prev = W.T @ dZ

        return grads

    def update_params(self, parameters, grads, learning_rate):
        L = len(parameters) // 2
        for l in range(1, L + 1):
            parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]
            parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]
        return parameters

    def initialize_momentum(self, parameters):
        L = len(parameters) // 2
        v = {}
        for l in range(1, L + 1):
            v["dW" + str(l)] = np.zeros_like(parameters["W" + str(l)])
            v["db" + str(l)] = np.zeros_like(parameters["b" + str(l)])
        return v

    def update_params_momentum(self, parameters, grads, v, learning_rate=0.01, beta=0.9):
        L = len(parameters) // 2
        for l in range(1, L + 1):
            v["dW" + str(l)] = beta * v["dW" + str(l)] + (1 - beta) * grads["dW" + str(l)]
            v["db" + str(l)] = beta * v["db" + str(l)] + (1 - beta) * grads["db" + str(l)]
            parameters["W" + str(l)] -= learning_rate * v["dW" + str(l)]
            parameters["b" + str(l)] -= learning_rate * v["db" + str(l)]
        return parameters, v

    def initialize_adam(self, parameters):
        L = len(parameters) // 2
        v, s = {}, {}
        for l in range(1, L + 1):
            v['dW' + str(l)] = np.zeros_like(parameters['W' + str(l)])
            v['db' + str(l)] = np.zeros_like(parameters['b' + str(l)])
            s['dW' + str(l)] = np.zeros_like(parameters['W' + str(l)])
            s['db' + str(l)] = np.zeros_like(parameters['b' + str(l)])
        return v, s

    def update_params_adam(self, parameters, grads, v, s, t, beta1=0.9, beta2=0.999, epsilon=1e-8):
        L = len(parameters) // 2
        for l in range(1, L + 1):
            v['dW' + str(l)] = beta1 * v['dW' + str(l)] + (1 - beta1) * grads['dW' + str(l)]
            v['db' + str(l)] = beta1 * v['db' + str(l)] + (1 - beta1) * grads['db' + str(l)]
            s['dW' + str(l)] = beta2 * s['dW' + str(l)] + (1 - beta2) * (grads['dW' + str(l)] ** 2)
            s['db' + str(l)] = beta2 * s['db' + str(l)] + (1 - beta2) * (grads['db' + str(l)] ** 2)

            v_corrected_dW = v['dW' + str(l)] / (1 - beta1 ** t)
            v_corrected_db = v['db' + str(l)] / (1 - beta1 ** t)
            s_corrected_dW = s['dW' + str(l)] / (1 - beta2 ** t)
            s_corrected_db = s['db' + str(l)] / (1 - beta2 ** t)

            parameters['W' + str(l)] -= self.lr * (v_corrected_dW / (np.sqrt(s_corrected_dW) + epsilon))
            parameters['b' + str(l)] -= self.lr * (v_corrected_db / (np.sqrt(s_corrected_db) + epsilon))
        return parameters, v, s

    def one_hot_encode(self, y, num_classes=10):
        return np.eye(num_classes)[y].T

    def fit(self, X_train, y_train, epochs=25, batch_size=64, optimizer=None, patience=5):
            layer_size = [self.input_size, self.hidden1, self.hidden2, self.output_size]
            self.initialize_params(layer_size)

            if y_train.ndim == 1:
                y_train = self.one_hot_encode(y_train)

            if optimizer == "adam":
                self.v, self.s = self.initialize_adam(self.parameters)
            elif optimizer == "sgd_momentum":
                self.v = self.initialize_momentum(self.parameters)

            for epoch in range(epochs):
                indices = np.arange(X_train.shape[0])
                np.random.shuffle(indices)
                X_train, y_train = X_train[indices], y_train[:, indices]

                epoch_loss = 0
                for i in range(0, X_train.shape[0], batch_size):
                    X_batch = X_train[i:i + batch_size].T
                    y_batch = y_train[:, i:i + batch_size]

                    yhat, caches = self.forward_propagation(X_batch, self.parameters, training=True)
                    loss = self.cost_func(y_batch, yhat, parameters=self.parameters, lambd=self.lambd)
                    grads = self.back_prop(y_batch, yhat, caches, self.parameters, lambd=self.lambd)

                    if optimizer == "adam":
                        self.parameters, self.v, self.s = self.update_params_adam(self.parameters, grads, self.v, self.s, self.t)
                        self.t += 1
                    elif optimizer == "sgd_momentum":
                        self.parameters, self.v = self.update_params_momentum(self.parameters, grads, self.v, learning_rate=self.lr)
                    else:
                        self.parameters = self.update_params(self.parameters, grads, self.lr)

                    epoch_loss += loss

                avg_loss = epoch_loss / (X_train.shape[0] // batch_size)
                self.train_losses.append(avg_loss)

                yhat_train, _ = self.forward_propagation(X_train.T, self.parameters, training=False)
                preds_train = np.argmax(yhat_train, axis=0)
                labels_train = np.argmax(y_train, axis=0) if y_train.ndim > 1 else y_train
                train_accuracy = np.mean(preds_train == labels_train)

                print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Train Accuracy: {train_accuracy:.4f} ")

                if train_accuracy > self.best_accuracy:
                    self.best_accuracy = train_accuracy
                    self.best_params = {k: v.copy() for k, v in self.parameters.items()}
                    self.epochs_no_improve = 0
                else:
                    self.epochs_no_improve += 1
                    if self.epochs_no_improve >= patience:
                        print("Early stopping triggered based on train accuracy.")
                        break
    def predict(self, X):
        yhat, _ = self.forward_propagation(X.T, self.parameters, training=False)
        preds = np.argmax(yhat, axis=0)
        return preds

    def score(self, X, y, use_best_params=True):
      params = self.best_params if use_best_params and self.best_params is not None else self.parameters
      yhat, _ = self.forward_propagation(X.T, params, training=False)
      preds = np.argmax(yhat, axis=0)

      if y.ndim > 1:
          labels = np.argmax(y, axis=0)
      else:
          labels = y

      accuracy = np.mean(preds == labels)
      return float(accuracy)

model = MNISTClassifier(input_size=784, hidden1=196, hidden2=64, output_size=10)

model.fit(X_train,y_train,epochs=20)

y_pred = model.predict(X_test)

model.score(X_test,y_test)