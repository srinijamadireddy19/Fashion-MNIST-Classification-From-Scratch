# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1olDbgREmtHdufvsxSgFYGWOhp-w3hJSJ
"""

import numpy as np

from tensorflow.keras.datasets import fashion_mnist

data = fashion_mnist.load_data()

from sklearn.model_selection import train_test_split

(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)

X_train = X_train.reshape(-1,28*28) /255.0
X_test = X_test.reshape(-1,28*28) /255.0

np.size(np.unique(y_train))

# Creating batches
def create_batches(X,y,batch_size=64):
  m = X.shape[0]
  num_batches = m//batch_size
  batches =[]
  indices = np.random.permutation(m)
  X_shuffled = X[indices]
  y_shuffled = y[indices]

  for i in range(num_batches):
    X_batch = X_shuffled[i*batch_size:(i+1)*batch_size]
    y_batch = y_shuffled[i*batch_size:(i+1)*batch_size]
    batches.append((X_batch,y_batch))

  if m%batch_size != 0:
    X_batch = X_shuffled[num_batches*batch_size:]
    y_batch = y_shuffled[num_batches*batch_size:]
    batches.append((X_batch,y_batch))

  return batches

#Initialising w,b
def initialise_params(layer_size):

  parameters = {}
  L = len(layer_size) - 1

  for l in range(1,L+1):
    parameters['W'+str(l)] = np.random.randn(layer_size[l],layer_size[l-1]) * np.sqrt(2.0/layer_size[l-1])
    parameters['b'+str(l)] = np.zeros((layer_size[l],1))

  return parameters

#Dropout func for fwd and bwd
def apply_dropout_fwd(A,keep_prob):
  d = (np.random.rand(*A.shape) < keep_prob).astype(float)
  A = A*d
  A /= keep_prob
  return A,d

def apply_dropout_bwd(dA,d,keep_prob):
  dA = dA * d
  dA /= keep_prob
  return dA

#Activations function

def relu(X):
  return np.maximum(0,X)

def softmax(X):
  return np.exp(X) / np.sum(np.exp(X))

def activation_function(activation, Z):
    if activation == 'relu':
        A = np.maximum(0, Z)
    elif activation == 'softmax':
        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)
        A = np.exp(Z_shifted) / np.sum(np.exp(Z_shifted), axis=0, keepdims=True)
    elif activation == 'tanh':
        A = np.tanh(Z)
    cache = Z  # keep Z for backprop
    return A, cache

#linear fwd prop
def linear_fwd_prop(A,W,b):
  Z = np.dot(W,A) + b
  cache = (A,W,b)
  return Z,cache

#Forward Propagation

def forward_propagation(X,parameters,training=True):
  caches = []
  L = len(parameters) //2
  A =X

  for l in range(1,L):
    Z,lin_cache = linear_fwd_prop(A,parameters['W'+str(l)],parameters['b'+str(l)])
    A,act_cache = activation_function('relu',Z)
    if training:
      A, d = apply_dropout_fwd(A, 0.5)
    else:
      d = None
    caches.append((lin_cache,act_cache,d))

  Z,lin_cache = linear_fwd_prop(A,parameters['W'+str(L)],parameters['b'+str(L)])
  A,act_cache = activation_function('softmax',Z)
  caches.append((lin_cache,act_cache,None))

  return A,caches

#cross entropy loss with l2 regularisation

def cost_func(y,yhat,parameters=None, lambd=0.0):
  m = y.shape[1]  # batch size
  loss = -np.sum(y * np.log(yhat+1e-7)) / m
  L2_term = 0
  if parameters and lambd > 0:
      L = len(parameters) // 2
      for l in range(1, L + 1):
          L2_term += np.sum(np.square(parameters['W'+str(l)]))
      L2_term *= (lambd / (2 * m))

  return loss + L2_term

#backward activations

def relu_backward(dA, Z):
    return dA * (Z > 0)

def softmax_backward(dA,Z):
  dZ = np.array(dA, copy=True)
  return dZ

#Backward Propagation

def back_prop(y,yhat,caches,parameters,lambd=0.1, grad_clip_value=5):
  grads = {}
  L = len(caches)
  m = y.shape[1]

  linear_cache, act_cache,d = caches[L-1]
  A_prev,W,b = linear_cache
  Z = act_cache
  dZ = yhat-y

  # Clip gradients
  dZ = np.clip(dZ, -grad_clip_value, grad_clip_value)

  grads['dW'+str(L)] = (dZ @ A_prev.T) /m + (lambd / m) * W
  grads['db'+str(L)] = np.sum(dZ,axis=1,keepdims=True)/m
  dA_prev = W.T @ dZ

  for l in reversed(range(L-1)):
    linear_cache,act_cache,d = caches[l]
    A_prev,W,b = linear_cache
    Z = act_cache
    if d is not None:
      dA = apply_dropout_bwd(dA_prev,d,0.5)
    else:
      dA = dA_prev
    dZ = relu_backward(dA,Z)

    # Clip gradients
    dZ = np.clip(dZ, -grad_clip_value, grad_clip_value)

    grads['dW'+str(l+1)] = (dZ @ A_prev.T) /m + (lambd / m) * W
    grads['db'+str(l+1)] = np.sum(dZ,axis=1,keepdims=True)/m
    dA_prev = W.T @ dZ
  return grads

#Update Paramters

def update_params(parameters,grads,learning_rate):
  L = len(parameters) // 2

  for l in range(1,L+1):
    parameters['W'+str(l)] = parameters['W'+str(l)] - learning_rate * grads['dW'+str(l)]
    parameters['b'+str(l)] = parameters['b'+str(l)] - learning_rate * grads['db'+str(l)]

  return parameters

#one hot encoding
def one_hot_encode(y, num_classes=10):
    return np.eye(num_classes)[y]

def initialize_momentum(parameters):
    L = len(parameters) // 2
    v = {}
    for l in range(1, L + 1):
        v["dW" + str(l)] = np.zeros_like(parameters["W" + str(l)])
        v["db" + str(l)] = np.zeros_like(parameters["b" + str(l)])
    return v

def update_params_momentum(parameters, grads, v, learning_rate=0.01, beta=0.9):
    L = len(parameters) // 2
    for l in range(1, L + 1):
        v["dW" + str(l)] = (beta * v["dW" + str(l)]) + (1 - beta) * grads["dW" + str(l)]
        v["db" + str(l)] = (beta * v["db" + str(l)]) + (1 - beta) * grads["db" + str(l)]
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * v["dW" + str(l)]
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * v["db" + str(l)]
    return parameters, v

def initialize_adam(parameters):
  L = len(parameters) // 2
  v = {}
  s = {}
  for l in range(1,L+1):
    v['dW' + str(l)] = np.zeros_like(parameters['W' + str(l)])
    v['db' + str(l)] = np.zeros_like(parameters['b' + str(l)])
    s['dW' + str(l)] = np.zeros_like(parameters['W' + str(l)])
    s['db' + str(l)] = np.zeros_like(parameters['b' + str(l)])
  return v, s

def update_params_adam(parameters,grad,v,s,t,beta1=0.9,beta2=0.99,epsilon = 1e-8):
  L = len(parameters)//2
  v_corrected = {}
  s_corrected = {}

  for l in range(1,L+1):
    v['dW' + str(l)] = beta1 * v['dW' + str(l)] + (1 - beta1) * grads['dW' + str(l)]
    v['db' + str(l)] = beta1 * v['db' + str(l)] + (1 - beta1) * grads['db' + str(l)]

    s['dW' + str(l)] = beta2 * s['dW' + str(l)] + (1 - beta2) * np.square(grads['dW' + str(l)])
    s['db' + str(l)] = beta2 * s['db' + str(l)] + (1 - beta2) * np.square(grads['db' + str(l)])

    v_corrected['dW' + str(l)] = v['dW' + str(l)] / (1 - beta1 ** t)
    v_corrected['db' + str(l)] = v['db' + str(l)] / (1 - beta1 ** t)
    s_corrected['dW' + str(l)] = s['dW' + str(l)] / (1 - beta2 ** t)
    s_corrected['db' + str(l)] = s['db' + str(l)] / (1 - beta2 ** t)

    parameters['W' + str(l)] -= learning_rate * (v_corrected['dW' + str(l)] / (np.sqrt(s_corrected['dW' + str(l)])+ epsilon))
    parameters['b' + str(l)] -= learning_rate * (v_corrected['db' + str(l)] / np.sqrt(s_corrected['db' + str(l)]+ epsilon))

    return parameters,v,s



#Arguments
input_size = 784
hidden1 = 196
hidden2 = 64
output_size = 10
epochs = 25  # Increase epochs for early stopping
batch_size = 64
learning_rate = 0.2
learning_rate_decay = 0.95
patience = 5
best_accuracy = 0
epochs_no_improve = 0
best_params = None
lambd = 0.01
t = 0
optimizer = None
train_losses = []
train_accuracies = []
test_accuracies = []

#training loop
layer_size = [input_size, hidden1, hidden2, output_size]
parameters = initialise_params(layer_size)


if y_train.ndim == 1:
    y_train = one_hot_encode(y_train, 10)
if y_test.ndim == 1:
    y_test = one_hot_encode(y_test, 10)


for epoch in range(epochs):
  batches = create_batches(X_train,y_train,batch_size)
  epoch_loss = 0
  train_accuracy = 0

  for x_batch, y_batch in batches:
    x_batch = x_batch.T
    y_batch = y_batch.T

    yhat,caches = forward_propagation(x_batch,parameters)

    loss = cost_func(y_batch,yhat,parameters=parameters,lambd=lambd)
    epoch_loss += loss
    grads = back_prop(y_batch,yhat,caches,parameters)

    if optimizer == "adam":
      t += 1
      v, s = initialize_adam(parameters)
      parameters, v, s = update_params_adam(parameters, grads, v, s, t, learning_rate)
    elif optimizer == "sgd_momentum":
      v = initialize_momentum(parameters)
      parameters, v = update_params_momentum(parameters, grads, v, learning_rate=learning_rate)
    else:
      parameters = update_params(parameters, grads, learning_rate)
  yhat_train, _ = forward_propagation(X_train.T, parameters, training=False)
  preds = np.argmax(yhat_train, axis=0)
  labels = np.argmax(y_train.T, axis=0)
  train_accuracy = np.mean(preds == labels)

  yhat_test, _ = forward_propagation(X_test.T, parameters, training=False)
  test_preds = np.argmax(yhat_test, axis=0)
  test_labels = np.argmax(y_test.T, axis=0)
  test_accuracy = np.mean(test_preds == test_labels)

  avg_loss = epoch_loss / len(batches)
  train_losses.append(avg_loss)
  train_accuracies.append(train_accuracy)
  test_accuracies.append(test_accuracy)

  if test_accuracy > best_accuracy:
        best_accuracy = test_accuracy
        best_params = {k: v.copy() for k, v in parameters.items()}
        epochs_no_improve = 0
  else:
        epochs_no_improve += 1

  avg_loss = epoch_loss/len(batches)
  print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - Test Accuracy: {test_accuracy:.4f}")

#test accuracy
yhat_test, _ = forward_propagation(X_test.T, parameters)
preds_test = np.argmax(yhat_test, axis=0)
labels_test = np.argmax(y_test.T, axis=0)
test_acc = np.mean(preds_test == labels_test)

print(f"Test Accuracy: {test_acc:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(20, 8))

plt.subplot(1,2,1)
plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(test_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Test Accuracy')



